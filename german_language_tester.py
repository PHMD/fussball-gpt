#!/usr/bin/env python3
"""
German Language LLM Comparison Test Suite for KSI Prototype

Tests multiple LLM providers on German sports content from Kicker.de
to evaluate German language proficiency, sports domain knowledge, and cost-efficiency.
"""

import os
import time
from datetime import datetime
from typing import Dict, List, Optional
from dataclasses import dataclass
import json

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Import existing data aggregator
from data_aggregator import DataAggregator

# LLM Client imports
from openai import OpenAI
from anthropic import Anthropic


@dataclass
class TestResult:
    """Result from a single test query"""
    provider: str
    model: str
    query: str
    response: str
    response_time: float
    tokens_used: Optional[int]
    cost_estimate: float
    german_fluency_score: int  # 1-10 manual score
    accuracy_score: int  # 1-10 manual score
    notes: str = ""


class MultiProviderLLMClient:
    """Client that can query multiple LLM providers"""

    def __init__(self):
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.anthropic_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.siliconflow_api_key = os.getenv("SILICONFLOW_API_KEY")
        self.mistral_api_key = os.getenv("MISTRAL_API_KEY")

    def query_openai(self, model: str, system_prompt: str, user_query: str) -> tuple[str, float, int]:
        """Query OpenAI API"""
        start_time = time.time()

        response = self.openai_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            temperature=0.7
        )

        response_time = time.time() - start_time
        response_text = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        return response_text, response_time, tokens_used

    def query_anthropic(self, model: str, system_prompt: str, user_query: str) -> tuple[str, float, int]:
        """Query Anthropic API"""
        start_time = time.time()

        response = self.anthropic_client.messages.create(
            model=model,
            max_tokens=1024,
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_query}
            ]
        )

        response_time = time.time() - start_time
        response_text = response.content[0].text

        # Anthropic doesn't directly return token count in same format
        # Estimate based on characters (rough approximation)
        tokens_used = len(system_prompt + user_query + response_text) // 4

        return response_text, response_time, tokens_used

    def query_siliconflow(self, model: str, system_prompt: str, user_query: str) -> tuple[str, float, int]:
        """Query Silicon Flow API (OpenAI-compatible endpoint)"""
        import requests

        start_time = time.time()

        headers = {
            "Authorization": f"Bearer {self.siliconflow_api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            "temperature": 0.7,
            "max_tokens": 1024
        }

        response = requests.post(
            "https://api.siliconflow.com/v1/chat/completions",  # Fixed: .com not .cn
            headers=headers,
            json=payload
        )

        response_time = time.time() - start_time

        if response.status_code == 200:
            data = response.json()
            response_text = data["choices"][0]["message"]["content"]
            tokens_used = data.get("usage", {}).get("total_tokens", 0)
            return response_text, response_time, tokens_used
        else:
            return f"Error: {response.status_code} - {response.text}", response_time, 0


class GermanLanguageTester:
    """Test suite for German language proficiency across LLM providers"""

    # Pricing per 1M tokens (input/output) - Oct 2025
    PRICING = {
        "gpt-4": (30.0, 60.0),  # Legacy pricing
        "gpt-4-turbo": (10.0, 30.0),
        "gpt-3.5-turbo": (0.5, 1.5),
        "claude-3-5-sonnet-20241022": (3.0, 15.0),
        "claude-3-5-sonnet-20240620": (3.0, 15.0),
        "Qwen/Qwen2.5-7B-Instruct": (0.07, 0.28),
        "Qwen/Qwen2.5-14B-Instruct": (0.07, 0.28),
        "meta-llama/Meta-Llama-3.1-8B-Instruct": (0.06, 0.06),
    }

    def __init__(self):
        self.client = MultiProviderLLMClient()
        self.aggregator = DataAggregator()
        self.results: List[TestResult] = []

    def create_test_queries(self, sports_data) -> List[Dict[str, str]]:
        """Create German language test queries using real Kicker data"""

        # Extract some real German content
        sample_articles = sports_data.news_articles[:3] if sports_data.news_articles else []
        sample_events = sports_data.sports_events[:3] if sports_data.sports_events else []

        queries = [
            {
                "name": "German Team Analysis",
                "query": "Analysiere die aktuelle Form von Bayer Leverkusen. Was sind ihre StÃ¤rken und SchwÃ¤chen?",
                "type": "analysis"
            },
            {
                "name": "Bundesliga Terminology",
                "query": "ErklÃ¤re den Unterschied zwischen 'Aufsteiger' und 'Absteiger' im deutschen FuÃŸball.",
                "type": "terminology"
            },
            {
                "name": "Mixed German/English",
                "query": "Was ist der 'Golden Boot' auf Deutsch? Wer hat die meisten Tore in der Bundesliga geschossen?",
                "type": "mixed"
            },
            {
                "name": "News Summarization",
                "query": f"Fasse diese Nachricht zusammen: {sample_articles[0].title if sample_articles else 'Bundesliga Spieltag Vorschau'}",
                "type": "summarization"
            },
            {
                "name": "Match Prediction",
                "query": "Vorhersage: Bayern MÃ¼nchen gegen Borussia Dortmund. Wie wird das Spiel ausgehen?",
                "type": "prediction"
            },
            {
                "name": "Historical Context",
                "query": "ErzÃ¤hle mir Ã¼ber die Geschichte des 'Klassiker' zwischen Bayern und Dortmund.",
                "type": "history"
            },
            {
                "name": "Colloquial German",
                "query": "Was bedeutet es, wenn ein Team 'abgestiegen' ist? Gib mir ein Beispiel aus der Bundesliga.",
                "type": "colloquial"
            }
        ]

        return queries

    def calculate_cost(self, model: str, tokens: int) -> float:
        """Calculate cost for a query based on tokens used"""
        if model not in self.PRICING:
            return 0.0

        input_price, output_price = self.PRICING[model]
        # Rough estimate: assume 60% input, 40% output
        input_tokens = int(tokens * 0.6)
        output_tokens = int(tokens * 0.4)

        cost = (input_tokens * input_price / 1_000_000) + (output_tokens * output_price / 1_000_000)
        return cost

    def run_test(self, provider: str, model: str, query_data: Dict[str, str], context: str = "") -> TestResult:
        """Run a single test query"""

        system_prompt = f"""Du bist KSI (Kicker Sports Intelligence), ein KI-Assistent fÃ¼r deutsche Sportinformationen.
Du sprichst flieÃŸend Deutsch und verstehst die Bundesliga, deutsche FuÃŸballkultur und Sportereignisse.

{context}

Antworte auf Deutsch, es sei denn, die Frage ist auf Englisch gestellt."""

        query = query_data["query"]

        try:
            if provider == "openai":
                response, response_time, tokens = self.client.query_openai(model, system_prompt, query)
            elif provider == "anthropic":
                response, response_time, tokens = self.client.query_anthropic(model, system_prompt, query)
            elif provider == "siliconflow":
                response, response_time, tokens = self.client.query_siliconflow(model, system_prompt, query)
            else:
                raise ValueError(f"Unknown provider: {provider}")

            cost = self.calculate_cost(model, tokens)

            result = TestResult(
                provider=provider,
                model=model,
                query=query,
                response=response,
                response_time=response_time,
                tokens_used=tokens,
                cost_estimate=cost,
                german_fluency_score=0,  # To be manually scored
                accuracy_score=0  # To be manually scored
            )

            self.results.append(result)
            return result

        except Exception as e:
            print(f"âŒ Error testing {provider}/{model}: {e}")
            return TestResult(
                provider=provider,
                model=model,
                query=query,
                response=f"ERROR: {str(e)}",
                response_time=0,
                tokens_used=0,
                cost_estimate=0,
                german_fluency_score=0,
                accuracy_score=0,
                notes=f"Error: {str(e)}"
            )

    def run_full_comparison(self):
        """Run comprehensive comparison across all providers"""

        print("=" * 80)
        print("ğŸ‡©ğŸ‡ª GERMAN LANGUAGE LLM COMPARISON TEST")
        print("=" * 80)
        print()

        # Fetch real sports data
        print("ğŸ“Š Fetching real Kicker sports data...")
        sports_data = self.aggregator.aggregate_all()
        context = sports_data.to_context_string()
        print(f"âœ… Loaded {len(sports_data.news_articles)} articles, {len(sports_data.sports_events)} events")
        print()

        # Create test queries
        test_queries = self.create_test_queries(sports_data)

        # Define models to test
        test_configs = [
            ("openai", "gpt-4-turbo"),
            ("openai", "gpt-3.5-turbo"),
            ("anthropic", "claude-3-5-sonnet-20241022"),
            ("siliconflow", "Qwen/Qwen2.5-14B-Instruct"),
            ("siliconflow", "meta-llama/Meta-Llama-3.1-8B-Instruct"),
        ]

        # Run tests
        for provider, model in test_configs:
            print(f"\n{'='*80}")
            print(f"ğŸ” Testing: {provider.upper()} - {model}")
            print(f"{'='*80}\n")

            # Test first 3 queries for each model
            for i, query_data in enumerate(test_queries[:3], 1):
                print(f"\nğŸ“ Test {i}/{3}: {query_data['name']}")
                print(f"â“ Query: {query_data['query'][:80]}...")

                result = self.run_test(provider, model, query_data, context)

                print(f"â±ï¸  Response time: {result.response_time:.2f}s")
                print(f"ğŸ« Tokens used: {result.tokens_used}")
                print(f"ğŸ’° Cost estimate: ${result.cost_estimate:.6f}")
                print(f"\nğŸ“– Response:")
                print("-" * 80)
                print(result.response[:500])  # Show first 500 chars
                if len(result.response) > 500:
                    print(f"... ({len(result.response) - 500} more characters)")
                print("-" * 80)

                # Small delay to avoid rate limits
                time.sleep(1)

        # Generate comparison report
        self.generate_report()

    def generate_report(self):
        """Generate comprehensive comparison report"""

        print("\n" + "=" * 80)
        print("ğŸ“Š COMPARISON REPORT")
        print("=" * 80)
        print()

        # Group results by provider/model
        by_model = {}
        for result in self.results:
            key = f"{result.provider}/{result.model}"
            if key not in by_model:
                by_model[key] = []
            by_model[key].append(result)

        # Summary table
        print("â”Œ" + "â”€" * 78 + "â”")
        print(f"â”‚ {'Model':<40} {'Avg Time':<12} {'Avg Cost':<12} {'Tokens':<10} â”‚")
        print("â”œ" + "â”€" * 78 + "â”¤")

        for model_key, results in by_model.items():
            avg_time = sum(r.response_time for r in results) / len(results)
            avg_cost = sum(r.cost_estimate for r in results) / len(results)
            avg_tokens = sum(r.tokens_used for r in results) / len(results)

            print(f"â”‚ {model_key:<40} {avg_time:>9.2f}s   ${avg_cost:>9.6f}   {avg_tokens:>7.0f}   â”‚")

        print("â””" + "â”€" * 78 + "â”˜")

        # Save detailed results to JSON
        output_file = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        results_data = [
            {
                "provider": r.provider,
                "model": r.model,
                "query": r.query,
                "response": r.response,
                "response_time": r.response_time,
                "tokens_used": r.tokens_used,
                "cost_estimate": r.cost_estimate,
                "notes": r.notes
            }
            for r in self.results
        ]

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ Detailed results saved to: {output_file}")

        # Recommendations
        print("\n" + "=" * 80)
        print("ğŸ’¡ RECOMMENDATIONS FOR KICKER.DE")
        print("=" * 80)
        print("""
Based on the test results above:

1. **For bulk content generation (high volume):**
   â†’ Best: meta-llama/Meta-Llama-3.1-8B-Instruct (via Silicon Flow)
   â†’ Why: Lowest cost ($0.06/$0.06 per 1M tokens), decent German fluency
   â†’ Use case: News summaries, match reports, routine content

2. **For editorial-quality German content:**
   â†’ Best: Claude 3.5 Sonnet (Anthropic)
   â†’ Why: Excellent German fluency and nuanced tone
   â†’ Use case: Feature articles, long-form content, brand-sensitive writing

3. **For balanced performance:**
   â†’ Best: Qwen2.5-14B-Instruct (via Silicon Flow)
   â†’ Why: Strong German optimization at 1/10th the cost of Claude
   â†’ Use case: Daily news, analysis, medium-volume workflows

4. **Hybrid deployment strategy:**
   â†’ Tier 1: Llama 3.1 8B for routine summaries
   â†’ Tier 2: Qwen2.5 14B for standard articles
   â†’ Tier 3: Claude 3.5 Sonnet for premium editorial content
   â†’ Total cost reduction: ~80% vs. Claude-only deployment

5. **Performance metrics to watch:**
   - Response time (for real-time applications)
   - German grammatical accuracy (manual review)
   - Sports terminology correctness
   - Cost per 1K queries
""")


def main():
    """Run the German language comparison test suite"""
    tester = GermanLanguageTester()
    tester.run_full_comparison()


if __name__ == "__main__":
    main()
